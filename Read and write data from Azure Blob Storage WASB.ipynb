{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
        "\n",
        "You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
        "\n",
        "    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
        "\n",
        "This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a sample data\n",
        "\n",
        "Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "end_date = datetime.today()\n",
        "start_date = datetime.today() - relativedelta(months=6)\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:15:48.2549112Z",
              "execution_start_time": "2021-05-26T03:16:26.828594Z",
              "execution_finish_time": "2021-05-26T03:16:39.2155257Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 5 rows\n",
        "hol_df.show(5, truncate = False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:16:49.6262305Z",
              "execution_start_time": "2021-05-26T03:16:49.7571053Z",
              "execution_finish_time": "2021-05-26T03:16:55.959111Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------------------------+----------------------------+-------------+-----------------+-------------------+\n",
            "|countryOrRegion|holidayName                 |normalizeHolidayName        |isPaidTimeOff|countryRegionCode|date               |\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+-------------------+\n",
            "|Norway         |Søndag                      |Søndag                      |null         |NO               |2020-11-29 00:00:00|\n",
            "|Sweden         |Söndag                      |Söndag                      |null         |SE               |2020-11-29 00:00:00|\n",
            "|Scotland       |St. Andrew's Day            |St. Andrew's Day            |null         |null             |2020-11-30 00:00:00|\n",
            "|United Kingdom |St. Andrew's Day [Scotland] |St. Andrew's Day            |false        |GB               |2020-11-30 00:00:00|\n",
            "|Portugal       |Restauração da Independência|Restauração da Independência|null         |PT               |2020-12-01 00:00:00|\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+-------------------+\n",
            "only showing top 5 rows"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write data to Azure Storage Blob\n",
        "\n",
        "Synapse leverage **Shared access signature (SAS)** to access Azure Blob Storage. To avoid exposing SAS keys in the code, we recommend creating a new linked service in Synapse workspace to the Azure Blob Storage account you want to access.\n",
        "\n",
        "Follow these steps to add a new linked service for an Azure Blob Storage account:\n",
        "\n",
        "1. Open the [Azure Synapse Studio](https://web.azuresynapse.net/).\n",
        "2. Select **Manage** from the left panel and select **Linked services** under the **External connections**.\n",
        "3. Search **Azure Blob Storage** in the **New linked Service** panel on the right.\n",
        "4. Select **Continue**.\n",
        "5. Select the Azure Blob Storage Account to access and configure the linked service name. Suggest using **Account key** for the **Authentication method**.\n",
        "6. Select **Test connection** to validate the settings are correct.\n",
        "7. Select **Create** first and click **Publish all** to save your changes.\n",
        "\n",
        "You can access data on Azure Blob Storage with Synapse Spark via following URL:\n",
        "\n",
        "```wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/```\n",
        "\n",
        "Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Azure storage access info\n",
        "blob_account_name = 'samdbblob' # replace with your blob name\n",
        "blob_container_name = 'myblob' # replace with your container name\n",
        "blob_relative_path = 'myFolder/' # replace with your relative folder path\n",
        "linked_service_name = 'AzureBlobStorage1' # replace with your linked service name\n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "print(blob_sas_token)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:17:00.0663401Z",
              "execution_start_time": "2021-05-26T03:17:00.1796236Z",
              "execution_finish_time": "2021-05-26T03:17:02.2385169Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?sv=2020-02-10&ss=bf&srt=sco&se=2021-05-27T03%3A17%3A01Z&sp=rwdl&sig=O691x43VLj6hqezxKiyRczSTE7iM6o0rfEmotQ25EiY%3D"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Allow SPARK to access from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
        "print('Remote blob path: ' + wasbs_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:17:09.1033244Z",
              "execution_start_time": "2021-05-26T03:17:09.2044717Z",
              "execution_finish_time": "2021-05-26T03:17:11.2703844Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remote blob path: wasbs://myblob@samdbblob.blob.core.windows.net/myFolder/"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save a dataframe as Parquet, JSON or CSV\n",
        "If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
        "\n",
        "Dataframes can be saved in any format, regardless of the input format.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_path = wasbs_path + 'holiday.parquet'\n",
        "json_path = wasbs_path + 'holiday.json'\n",
        "csv_path = wasbs_path + 'holiday.csv'\n",
        "print('parquet file path: ' + parquet_path)\n",
        "print('json file path： ' + json_path)\n",
        "print('csv file path: ' + csv_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:17:18.4294403Z",
              "execution_start_time": "2021-05-26T03:17:18.5421306Z",
              "execution_finish_time": "2021-05-26T03:17:20.6106045Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parquet file path: wasbs://myblob@samdbblob.blob.core.windows.net/myFolder/holiday.parquet\n",
            "json file path： wasbs://myblob@samdbblob.blob.core.windows.net/myFolder/holiday.json\n",
            "csv file path: wasbs://myblob@samdbblob.blob.core.windows.net/myFolder/holiday.csv"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
        "hol_df.write.json(json_path, mode = 'overwrite')\n",
        "hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:17:26.5723757Z",
              "execution_start_time": "2021-05-26T03:17:26.688316Z",
              "execution_finish_time": "2021-05-26T03:18:43.0768624Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save a dataframe as text files\n",
        "If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text file path\n",
        "text_path = wasbs_path + 'holiday.txt'\n",
        "print('text file path: ' + text_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:18:51.2524811Z",
              "execution_start_time": "2021-05-26T03:18:51.3658773Z",
              "execution_finish_time": "2021-05-26T03:18:53.4222704Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text file path: wasbs://myblob@samdbblob.blob.core.windows.net/myFolder/holiday.txt"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Covert spark dataframe into RDD \n",
        "hol_RDD = hol_df.rdd\n",
        "type(hol_RDD)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:19:08.8950904Z",
              "execution_start_time": "2021-05-26T03:19:08.9946629Z",
              "execution_finish_time": "2021-05-26T03:19:11.0630012Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have an RDD, you can convert it to a text file like the following:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        " # Save RDD as text file\n",
        "hol_RDD.saveAsTextFile(text_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:19:12.7289334Z",
              "execution_start_time": "2021-05-26T03:19:12.8354098Z",
              "execution_finish_time": "2021-05-26T03:19:37.6965447Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data from Azure Storage Blob\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataframe from parquet files\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet = spark.read.parquet(parquet_path)\r\n",
        "df_parquet.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:19:43.7879649Z",
              "execution_start_time": "2021-05-26T03:19:43.9202419Z",
              "execution_finish_time": "2021-05-26T03:19:52.1722825Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+-------------+-----------------+-------------------+\n",
            "|countryOrRegion|         holidayName|normalizeHolidayName|isPaidTimeOff|countryRegionCode|               date|\n",
            "+---------------+--------------------+--------------------+-------------+-----------------+-------------------+\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-11-29 00:00:00|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-11-29 00:00:00|\n",
            "|       Scotland|    St. Andrew's Day|    St. Andrew's Day|         null|             null|2020-11-30 00:00:00|\n",
            "| United Kingdom|St. Andrew's Day ...|    St. Andrew's Day|        false|               GB|2020-11-30 00:00:00|\n",
            "|       Portugal|Restauração da In...|Restauração da In...|         null|               PT|2020-12-01 00:00:00|\n",
            "|        Finland|    Itsenäisyyspäivä|    Itsenäisyyspäivä|         null|               FI|2020-12-06 00:00:00|\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-12-06 00:00:00|\n",
            "|          Spain|Día de la constit...|Día de la constit...|         null|               ES|2020-12-06 00:00:00|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-12-06 00:00:00|\n",
            "|      Argentina|La Inmaculada Con...|La Inmaculada Con...|         null|               AR|2020-12-08 00:00:00|\n",
            "|        Austria|    Mariä Empfängnis|    Mariä Empfängnis|         null|               AT|2020-12-08 00:00:00|\n",
            "|       Colombia|La Inmaculada Con...|La Inmaculada Con...|         null|               CO|2020-12-08 00:00:00|\n",
            "|          Italy|Immacolata Concez...|Immacolata Concez...|         null|               IT|2020-12-08 00:00:00|\n",
            "|       Portugal| Imaculada Conceição| Imaculada Conceição|         null|               PT|2020-12-08 00:00:00|\n",
            "|          Spain|La Inmaculada Con...|La Inmaculada Con...|         null|               ES|2020-12-08 00:00:00|\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-12-13 00:00:00|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-12-13 00:00:00|\n",
            "|   South Africa|Day of Reconcilia...|Day of Reconcilia...|         null|               ZA|2020-12-16 00:00:00|\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-12-20 00:00:00|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-12-20 00:00:00|\n",
            "+---------------+--------------------+--------------------+-------------+-----------------+-------------------+\n",
            "only showing top 20 rows"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataframe from JSON files\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_json = spark.read.json(json_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "mei",
              "session_id": 30,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-29T08:19:32.5152165Z",
              "execution_start_time": "2021-03-29T08:19:45.1389722Z",
              "execution_finish_time": "2021-03-29T08:19:47.2190643Z"
            },
            "text/plain": "StatementMeta(mei, 30, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataframe from CSV files\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
        "df_csv.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:20:00.1337223Z",
              "execution_start_time": "2021-05-26T03:20:00.2365415Z",
              "execution_finish_time": "2021-05-26T03:20:06.4376741Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+-------------+-----------------+--------------------+\n",
            "|countryOrRegion|         holidayName|normalizeHolidayName|isPaidTimeOff|countryRegionCode|                date|\n",
            "+---------------+--------------------+--------------------+-------------+-----------------+--------------------+\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-11-29T00:00:...|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-11-29T00:00:...|\n",
            "|       Scotland|    St. Andrew's Day|    St. Andrew's Day|         null|             null|2020-11-30T00:00:...|\n",
            "| United Kingdom|St. Andrew's Day ...|    St. Andrew's Day|        false|               GB|2020-11-30T00:00:...|\n",
            "|       Portugal|Restauração da In...|Restauração da In...|         null|               PT|2020-12-01T00:00:...|\n",
            "|        Finland|    Itsenäisyyspäivä|    Itsenäisyyspäivä|         null|               FI|2020-12-06T00:00:...|\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-12-06T00:00:...|\n",
            "|          Spain|Día de la constit...|Día de la constit...|         null|               ES|2020-12-06T00:00:...|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-12-06T00:00:...|\n",
            "|      Argentina|La Inmaculada Con...|La Inmaculada Con...|         null|               AR|2020-12-08T00:00:...|\n",
            "|        Austria|    Mariä Empfängnis|    Mariä Empfängnis|         null|               AT|2020-12-08T00:00:...|\n",
            "|       Colombia|La Inmaculada Con...|La Inmaculada Con...|         null|               CO|2020-12-08T00:00:...|\n",
            "|          Italy|Immacolata Concez...|Immacolata Concez...|         null|               IT|2020-12-08T00:00:...|\n",
            "|       Portugal| Imaculada Conceição| Imaculada Conceição|         null|               PT|2020-12-08T00:00:...|\n",
            "|          Spain|La Inmaculada Con...|La Inmaculada Con...|         null|               ES|2020-12-08T00:00:...|\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-12-13T00:00:...|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-12-13T00:00:...|\n",
            "|   South Africa|Day of Reconcilia...|Day of Reconcilia...|         null|               ZA|2020-12-16T00:00:...|\n",
            "|         Norway|              Søndag|              Søndag|         null|               NO|2020-12-20T00:00:...|\n",
            "|         Sweden|              Söndag|              Söndag|         null|               SE|2020-12-20T00:00:...|\n",
            "+---------------+--------------------+--------------------+-------------+-----------------+--------------------+\n",
            "only showing top 20 rows"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an RDD from text file\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "text = sc.textFile(text_path)\r\n",
        "print(text)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 19,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T03:20:27.7942579Z",
              "execution_start_time": "2021-05-26T03:20:27.9005676Z",
              "execution_finish_time": "2021-05-26T03:20:29.9709379Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 19, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wasbs://myblob@samdbblob.blob.core.windows.net/myFolder/holiday.txt MapPartitionsRDD[49] at textFile at NativeMethodAccessorImpl.java:0"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}