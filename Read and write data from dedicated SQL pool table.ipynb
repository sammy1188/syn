{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Access Synapse SQL table from Synapse Spark\n",
        "\n",
        "This notebook provides examples of how to read data from Synapse SQL into a Spark context and how to write the output of Spark jobs into an Synapse SQL table.\n",
        "\n",
        "\n",
        "## Limits\n",
        "- Scala is the only supported language by the Spark-SQL connector.\n",
        "- The Spark connector can only read colummns without space in its header in the sql pool.\n",
        "- Columns with time definition in the sql pool not yet supported.\n",
        "- You need to define a container on the workspace's primary or linked storage as the temp data folder.\n",
        "\n",
        "## Pre-requisites\n",
        "You need to be db_owner to read and write in sql pool. Ask your admin to run the following command with your AAD credential:\n",
        "\n",
        "    \n",
        "    EXEC sp_addrolemember 'db_owner', 'AAD@contoso.com'"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a sample data\n",
        "\n",
        "Let's first load the [Public Holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark \n",
        "# Load sample data from azure open dataset in pyspark\n",
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "end_date = datetime.today()\n",
        "start_date = datetime.today() - relativedelta(months=6)\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()\n",
        "\n",
        "print('Register the DataFrame as a SQL temporary view: source')\n",
        "hol_df.createOrReplaceTempView('source')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 20,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T04:55:04.1048413Z",
              "execution_start_time": "2021-05-26T04:58:52.5227422Z",
              "execution_finish_time": "2021-05-26T04:59:21.530134Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 20, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Register the DataFrame as a SQL temporary view: source"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Remove datetime from the data source\n",
        "val holiday_nodate = spark.sql(\"SELECT countryOrRegion, holidayName, normalizeHolidayName,isPaidTimeOff,countryRegionCode FROM source\")\n",
        "holiday_nodate.show(5,truncate = false)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 20,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T04:59:30.3846058Z",
              "execution_start_time": "2021-05-26T04:59:32.6392436Z",
              "execution_finish_time": "2021-05-26T04:59:45.0057136Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 20, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "holiday_nodate: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 3 more fields]\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+\n",
            "|countryOrRegion|holidayName                 |normalizeHolidayName        |isPaidTimeOff|countryRegionCode|\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+\n",
            "|Norway         |Søndag                      |Søndag                      |null         |NO               |\n",
            "|Sweden         |Söndag                      |Söndag                      |null         |SE               |\n",
            "|Scotland       |St. Andrew's Day            |St. Andrew's Day            |null         |null             |\n",
            "|United Kingdom |St. Andrew's Day [Scotland] |St. Andrew's Day            |false        |GB               |\n",
            "|Portugal       |Restauração da Independência|Restauração da Independência|null         |PT               |\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a Spark dataframe into your sql pool\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Write the dataframe into your sql pool\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "\n",
        "// val sql_pool_name = \"Built-in\" //fill in your sql pool name\n",
        "val sql_pool_name=\"TestDedicatedPool\" // this works, have to be dedicated pool, remember to turn it on...\n",
        "\n",
        "holiday_nodate.write.sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\", Constants.INTERNAL)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 20,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:03:37.6580431Z",
              "execution_start_time": "2021-05-26T05:03:37.7634607Z",
              "execution_finish_time": "2021-05-26T05:04:04.6514443Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 20, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
            "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
            "sql_pool_name: String = TestDedicatedPool\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now open Synapse object explorer and go to **Data**->**Databases**->**<your sql pool name>**->**Tables**, you will see the new **dbo.PublicHoliday** table show up there."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read from a SQL Pool table with Spark\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Read  the table we just created in the sql pool as a Spark dataframe\n",
        "val spark_read = spark.read.\n",
        "    sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\")\n",
        "spark_read.show(5, truncate = false)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 20,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:04:21.4882469Z",
              "execution_start_time": "2021-05-26T05:04:21.5885152Z",
              "execution_finish_time": "2021-05-26T05:04:31.9630577Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 20, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark_read: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 3 more fields]\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+\n",
            "|countryOrRegion|holidayName                 |normalizeHolidayName        |isPaidTimeOff|countryRegionCode|\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+\n",
            "|Norway         |Søndag                      |Søndag                      |null         |NO               |\n",
            "|Sweden         |Söndag                      |Söndag                      |null         |SE               |\n",
            "|Scotland       |St. Andrew's Day            |St. Andrew's Day            |null         |null             |\n",
            "|United Kingdom |St. Andrew's Day [Scotland] |St. Andrew's Day            |false        |GB               |\n",
            "|Portugal       |Restauração da Independência|Restauração da Independência|null         |PT               |\n",
            "+---------------+----------------------------+----------------------------+-------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "language": "Scala",
      "display_name": "Synapse Spark"
    },
    "language_info": {
      "name": "scala"
    },
    "kernel_info": {
      "name": "synapse_spark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}