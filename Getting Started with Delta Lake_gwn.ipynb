{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hitchhiker's Guide to Delta Lake (Python)\n",
        "\n",
        "This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
        "\n",
        "Here's what we will cover:\n",
        "* Create a table\n",
        "* Understanding meta-data\n",
        "* Read data\n",
        "* Update table data\n",
        "* Overwrite table data\n",
        "* Conditional update without overwrite\n",
        "* Read older versions of data using Time Travel\n",
        "* Write a stream of data to a table\n",
        "* Read a stream of changes from a table"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "Make sure you modify this as appropriate."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "session_id = random.randint(0,1000000)\n",
        "delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
        "\n",
        "delta_table_path"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:22:45.5422155Z",
              "execution_start_time": "2021-05-26T05:23:31.931297Z",
              "execution_finish_time": "2021-05-26T05:23:33.9898234Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/delta/delta-table-628038'"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a table\n",
        "To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
        "\n",
        "These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.range(0,5)\n",
        "data.show()\n",
        "data.write.format(\"delta\").save(delta_table_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:23:51.4251551Z",
              "execution_start_time": "2021-05-26T05:23:51.5365979Z",
              "execution_finish_time": "2021-05-26T05:24:14.2855677Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Meta-data\n",
        "\n",
        "In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:27:45.5055109Z",
              "execution_start_time": "2021-05-26T05:27:45.6388601Z",
              "execution_finish_time": "2021-05-26T05:27:47.6961097Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['{\"commitInfo\":{\"timestamp\":1622006642448,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"ErrorIfExists\",\"partitionBy\":\"[]\"},\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"6\",\"numOutputBytes\":\"2407\",\"numOutputRows\":\"5\"}}}', '{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}', '{\"metaData\":{\"id\":\"160059c0-e203-497b-87c0-9b1482c672dc\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\\\"type\\\\\":\\\\\"struct\\\\\",\\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"id\\\\\",\\\\\"type\\\\\":\\\\\"long\\\\\",\\\\\"nullable\\\\\":true,\\\\\"metadata\\\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1622006637555}}', '{\"add\":{\"path\":\"part-00000-6c39fa52-204b-4913-bb91-9dcdd253bf8a-c000.snappy.parquet\",\"partitionValues\":{},\"size\":262,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00003-97e93c7a-0d98-46f7-966b-2665dd402acd-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00006-2dbee441-90da-425f-a3f0-7add21a7408a-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00009-be0c764a-971a-423c-a99b-2004c8489f82-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00012-abc3d0c3-196d-4df9-a860-80fe305e13e8-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00015-396f5e6c-e3bb-4cf0-80e6-d8def3bc99dd-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}']"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data\n",
        "\n",
        "You read data in your Delta Lake table by specifying the path to the files."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:27:52.3521126Z",
              "execution_start_time": "2021-05-26T05:27:52.4559454Z",
              "execution_finish_time": "2021-05-26T05:27:54.520937Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  4|\n",
            "|  1|\n",
            "|  0|\n",
            "|  3|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update table data\n",
        "\n",
        "Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.range(5,10)\n",
        "data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:28:05.803478Z",
              "execution_start_time": "2021-05-26T05:28:05.9153839Z",
              "execution_finish_time": "2021-05-26T05:28:10.0494663Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "|  5|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:28:13.9000002Z",
              "execution_start_time": "2021-05-26T05:28:13.9943109Z",
              "execution_finish_time": "2021-05-26T05:28:16.0570434Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['{\"commitInfo\":{\"timestamp\":1622006886752,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"readVersion\":0,\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"6\",\"numOutputBytes\":\"2407\",\"numOutputRows\":\"5\"}}}', '{\"add\":{\"path\":\"part-00000-0be43343-faef-40d7-9111-ed269daf9aa5-c000.snappy.parquet\",\"partitionValues\":{},\"size\":262,\"modificationTime\":1622006886000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00003-b7a61481-635c-4bb1-bf3d-9fb75bab93fa-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006886000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00006-58decab5-67aa-4b31-92f6-72a6ec4fee0d-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006886000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00009-eb29daeb-b17e-4e08-bfb8-e9570c92d693-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006886000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00012-7a7c3a27-c2e6-4a5d-86aa-8a21c6dfd40f-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006886000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00015-cbf5070c-ad03-414a-ad97-e9b6ad54c87c-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006886000,\"dataChange\":true}}', '{\"remove\":{\"path\":\"part-00009-be0c764a-971a-423c-a99b-2004c8489f82-c000.snappy.parquet\",\"deletionTimestamp\":1622006886752,\"dataChange\":true}}', '{\"remove\":{\"path\":\"part-00000-6c39fa52-204b-4913-bb91-9dcdd253bf8a-c000.snappy.parquet\",\"deletionTimestamp\":1622006886752,\"dataChange\":true}}', '{\"remove\":{\"path\":\"part-00015-396f5e6c-e3bb-4cf0-80e6-d8def3bc99dd-c000.snappy.parquet\",\"deletionTimestamp\":1622006886752,\"dataChange\":true}}', '{\"remove\":{\"path\":\"part-00006-2dbee441-90da-425f-a3f0-7add21a7408a-c000.snappy.parquet\",\"deletionTimestamp\":1622006886752,\"dataChange\":true}}', '{\"remove\":{\"path\":\"part-00003-97e93c7a-0d98-46f7-966b-2665dd402acd-c000.snappy.parquet\",\"deletionTimestamp\":1622006886752,\"dataChange\":true}}', '{\"remove\":{\"path\":\"part-00012-abc3d0c3-196d-4df9-a860-80fe305e13e8-c000.snappy.parquet\",\"deletionTimestamp\":1622006886752,\"dataChange\":true}}', '{\"commitInfo\":{\"timestamp\":1622006642448,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"ErrorIfExists\",\"partitionBy\":\"[]\"},\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"6\",\"numOutputBytes\":\"2407\",\"numOutputRows\":\"5\"}}}', '{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}', '{\"metaData\":{\"id\":\"160059c0-e203-497b-87c0-9b1482c672dc\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\\\"type\\\\\":\\\\\"struct\\\\\",\\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"id\\\\\",\\\\\"type\\\\\":\\\\\"long\\\\\",\\\\\"nullable\\\\\":true,\\\\\"metadata\\\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1622006637555}}', '{\"add\":{\"path\":\"part-00000-6c39fa52-204b-4913-bb91-9dcdd253bf8a-c000.snappy.parquet\",\"partitionValues\":{},\"size\":262,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00003-97e93c7a-0d98-46f7-966b-2665dd402acd-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00006-2dbee441-90da-425f-a3f0-7add21a7408a-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00009-be0c764a-971a-423c-a99b-2004c8489f82-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00012-abc3d0c3-196d-4df9-a860-80fe305e13e8-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}', '{\"add\":{\"path\":\"part-00015-396f5e6c-e3bb-4cf0-80e6-d8def3bc99dd-c000.snappy.parquet\",\"partitionValues\":{},\"size\":429,\"modificationTime\":1622006642000,\"dataChange\":true}}']"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save as catalog tables\n",
        "\n",
        "Delta Lake can write to managed or external catalog tables."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Write data to a new managed catalog table.\n",
        "data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:28:19.0702498Z",
              "execution_start_time": "2021-05-26T05:28:19.1715657Z",
              "execution_finish_time": "2021-05-26T05:28:29.487691Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
        "spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:28:31.6544659Z",
              "execution_start_time": "2021-05-26T05:28:31.7636486Z",
              "execution_finish_time": "2021-05-26T05:28:33.8303844Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[]"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# List the 2 new tables.\n",
        "spark.sql(\"SHOW TABLES\").show()\n",
        "\n",
        "# Explore their properties.\n",
        "spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
        "spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:28:38.6019541Z",
              "execution_start_time": "2021-05-26T05:28:38.7170857Z",
              "execution_finish_time": "2021-05-26T05:28:40.7969424Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+-----------+\n",
            "|database|           tableName|isTemporary|\n",
            "+--------+--------------------+-----------+\n",
            "| default|              cities|      false|\n",
            "| default|             cities2|      false|\n",
            "| default|              movies|      false|\n",
            "| default|             movies2|      false|\n",
            "| default|        universities|      false|\n",
            "| default|            cities28|      false|\n",
            "| default|             dimdate|      false|\n",
            "| default|        dimgeography|      false|\n",
            "| default|          dimproduct|      false|\n",
            "| default|  dimproductcategory|      false|\n",
            "| default|dimproductsubcate...|      false|\n",
            "| default|   factinternetsales|      false|\n",
            "| default|   manageddeltatable|      false|\n",
            "| default|  externaldeltatable|      false|\n",
            "+--------+--------------------+-----------+\n",
            "\n",
            "+----------------------------+------------------------------------------------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                                                       |comment|\n",
            "+----------------------------+------------------------------------------------------------------------------------------------+-------+\n",
            "|id                          |bigint                                                                                          |null   |\n",
            "|                            |                                                                                                |       |\n",
            "|# Detailed Table Information|                                                                                                |       |\n",
            "|Database                    |default                                                                                         |       |\n",
            "|Table                       |manageddeltatable                                                                               |       |\n",
            "|Owner                       |trusted-service-user                                                                            |       |\n",
            "|Created Time                |Wed May 26 05:28:28 UTC 2021                                                                    |       |\n",
            "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                                                                    |       |\n",
            "|Created By                  |Spark 2.4.4.2.6.99.201-34744923                                                                 |       |\n",
            "|Type                        |MANAGED                                                                                         |       |\n",
            "|Provider                    |delta                                                                                           |       |\n",
            "|Table Properties            |[transient_lastDdlTime=1622006908]                                                              |       |\n",
            "|Location                    |abfss://syn@synstorest.dfs.core.windows.net/synapse/workspaces/wssyn/warehouse/manageddeltatable|       |\n",
            "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                              |       |\n",
            "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                                                |       |\n",
            "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                                       |       |\n",
            "|Storage Properties          |[serialization.format=1]                                                                        |       |\n",
            "+----------------------------+------------------------------------------------------------------------------------------------+-------+\n",
            "\n",
            "+----------------------------+--------------------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                                           |comment|\n",
            "+----------------------------+--------------------------------------------------------------------+-------+\n",
            "|id                          |bigint                                                              |null   |\n",
            "|                            |                                                                    |       |\n",
            "|# Detailed Table Information|                                                                    |       |\n",
            "|Database                    |default                                                             |       |\n",
            "|Table                       |externaldeltatable                                                  |       |\n",
            "|Owner                       |trusted-service-user                                                |       |\n",
            "|Created Time                |Wed May 26 05:28:32 UTC 2021                                        |       |\n",
            "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                                        |       |\n",
            "|Created By                  |Spark 2.4.4.2.6.99.201-34744923                                     |       |\n",
            "|Type                        |EXTERNAL                                                            |       |\n",
            "|Provider                    |DELTA                                                               |       |\n",
            "|Table Properties            |[transient_lastDdlTime=1622006912]                                  |       |\n",
            "|Location                    |abfss://syn@synstorest.dfs.core.windows.net/delta/delta-table-628038|       |\n",
            "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                  |       |\n",
            "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                    |       |\n",
            "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat           |       |\n",
            "|Storage Properties          |[serialization.format=1]                                            |       |\n",
            "+----------------------------+--------------------------------------------------------------------+-------+"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional update without overwrite\n",
        "\n",
        "Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "delta_table = DeltaTable.forPath(spark, delta_table_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:28:55.4999177Z",
              "execution_start_time": "2021-05-26T05:28:55.6037779Z",
              "execution_finish_time": "2021-05-26T05:28:57.675712Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Update every even value by adding 100 to it\n",
        "delta_table.update(\n",
        "  condition = expr(\"id % 2 == 0\"),\n",
        "  set = { \"id\": expr(\"id + 100\") })\n",
        "delta_table.toDF().show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:29:04.0689747Z",
              "execution_start_time": "2021-05-26T05:29:04.182014Z",
              "execution_finish_time": "2021-05-26T05:29:08.3129175Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  7|\n",
            "|108|\n",
            "|  9|\n",
            "|  5|\n",
            "|106|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete every even value\n",
        "delta_table.delete(\"id % 2 == 0\")\n",
        "delta_table.toDF().show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:29:19.4058106Z",
              "execution_start_time": "2021-05-26T05:29:19.5657342Z",
              "execution_finish_time": "2021-05-26T05:29:23.672169Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  7|\n",
            "|  9|\n",
            "|  5|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert (merge) new data\n",
        "new_data = spark.range(0,20).alias(\"newData\")\n",
        "\n",
        "delta_table.alias(\"oldData\")\\\n",
        "    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
        "    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
        "    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
        "    .execute()\n",
        "\n",
        "delta_table.toDF().show(100)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:29:26.3796416Z",
              "execution_start_time": "2021-05-26T05:29:26.4745106Z",
              "execution_finish_time": "2021-05-26T05:29:34.8050051Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  0|\n",
            "|  8|\n",
            "|  3|\n",
            "|  2|\n",
            "| -1|\n",
            "| 19|\n",
            "| 14|\n",
            "|  6|\n",
            "|  4|\n",
            "| 16|\n",
            "| 17|\n",
            "| 12|\n",
            "| -1|\n",
            "| 10|\n",
            "| 13|\n",
            "| -1|\n",
            "| 11|\n",
            "| 18|\n",
            "| 15|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## History\n",
        "Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table.history().show(20, 1000, False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:29:42.055905Z",
              "execution_start_time": "2021-05-26T05:29:42.2042665Z",
              "execution_finish_time": "2021-05-26T05:29:44.2598443Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+------+--------+---------+-------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|version|          timestamp|userId|userName|operation|                                                operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                                                                                                                                                                              operationMetrics|\n",
            "+-------+-------------------+------+--------+---------+-------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|      4|2021-05-26 05:29:30|  null|    null|    MERGE|                       [predicate -> (oldData.`id` = newData.`id`)]|null|    null|     null|          3|          null|        false|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 21, numTargetRowsInserted -> 17, numTargetRowsUpdated -> 3, numOutputRows -> 20, numSourceRows -> 20, numTargetFilesRemoved -> 3]|\n",
            "|      3|2021-05-26 05:29:20|  null|    null|   DELETE|[predicate -> [\"((`id` % CAST(2 AS BIGINT)) = CAST(0 AS BIGINT))\"]]|null|    null|     null|          2|          null|        false|                                                                                                                           [numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 0]|\n",
            "|      2|2021-05-26 05:29:06|  null|    null|   UPDATE| [predicate -> ((id#505L % cast(2 as bigint)) = cast(0 as bigint))]|null|    null|     null|          1|          null|        false|                                                                                                                           [numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 0]|\n",
            "|      1|2021-05-26 05:28:06|  null|    null|    WRITE|                             [mode -> Overwrite, partitionBy -> []]|null|    null|     null|          0|          null|        false|                                                                                                                                                   [numFiles -> 6, numOutputBytes -> 2407, numOutputRows -> 5]|\n",
            "|      0|2021-05-26 05:24:02|  null|    null|    WRITE|                         [mode -> ErrorIfExists, partitionBy -> []]|null|    null|     null|       null|          null|         true|                                                                                                                                                   [numFiles -> 6, numOutputBytes -> 2407, numOutputRows -> 5]|\n",
            "+-------+-------------------+------+--------+---------+-------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read older versions of data using Time Travel\n",
        "\n",
        "You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
        "\n",
        "Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:29:49.3519417Z",
              "execution_start_time": "2021-05-26T05:29:49.4456675Z",
              "execution_finish_time": "2021-05-26T05:29:53.57975Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  4|\n",
            "|  1|\n",
            "|  0|\n",
            "|  3|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a stream of data to a table\n",
        "\n",
        "You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
        "\n",
        "For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
        "\n",
        "In the cells below, here's what we are doing:\n",
        "\n",
        "1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
        "2. *Cell 30* Show the newly appended data\n",
        "3. *Cell 31* Inspect history\n",
        "4. *Cell 32* Stop the structured streaming job\n",
        "5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "streaming_df = spark.readStream.format(\"rate\").load()\n",
        "stream = streaming_df\\\n",
        "    .selectExpr(\"value as id\")\\\n",
        "    .writeStream\\\n",
        "    .format(\"delta\")\\\n",
        "    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
        "    .start(delta_table_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.6136965Z",
              "execution_start_time": "2021-02-23T05:32:54.5955644Z",
              "execution_finish_time": "2021-02-23T05:32:56.6322581Z"
            },
            "text/plain": "StatementMeta(small, 8, 18, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read a stream of changes from a table\n",
        "\n",
        "While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table.toDF().sort(col(\"id\").desc()).show(100)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SampleSpark",
              "session_id": 21,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-26T05:29:59.7787631Z",
              "execution_start_time": "2021-05-26T05:29:59.8797989Z",
              "execution_finish_time": "2021-05-26T05:30:01.9924125Z"
            },
            "text/plain": "StatementMeta(SampleSpark, 21, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "| 19|\n",
            "| 18|\n",
            "| 17|\n",
            "| 16|\n",
            "| 15|\n",
            "| 14|\n",
            "| 13|\n",
            "| 12|\n",
            "| 11|\n",
            "| 10|\n",
            "|  8|\n",
            "|  6|\n",
            "|  4|\n",
            "|  3|\n",
            "|  2|\n",
            "|  1|\n",
            "|  0|\n",
            "| -1|\n",
            "| -1|\n",
            "| -1|\n",
            "+---+"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.671101Z",
              "execution_start_time": "2021-02-23T05:33:00.8607701Z",
              "execution_finish_time": "2021-02-23T05:33:02.8979611Z"
            },
            "text/plain": "StatementMeta(small, 8, 20, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|version|          timestamp|       operation|                                                                  operationParameters|readVersion|                                                                                                                                                                                              operationMetrics|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|      6|2021-02-23 05:32:59|STREAMING UPDATE|[outputMode -> Append, queryId -> 348def45-706c-4555-ab12-178864ea7338, epochId -> 1]|          5|                                                                                                                        [numRemovedFiles -> 0, numOutputRows -> 3, numOutputBytes -> 1287, numAddedFiles -> 3]|\n|      5|2021-02-23 05:32:56|STREAMING UPDATE|[outputMode -> Append, queryId -> 348def45-706c-4555-ab12-178864ea7338, epochId -> 0]|          4|                                                                                                                         [numRemovedFiles -> 0, numOutputRows -> 0, numOutputBytes -> 262, numAddedFiles -> 1]|\n|      4|2021-02-23 05:32:40|           MERGE|                                         [predicate -> (oldData.`id` = newData.`id`)]|          3|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 21, numTargetRowsInserted -> 17, numTargetRowsUpdated -> 3, numOutputRows -> 20, numSourceRows -> 20, numTargetFilesRemoved -> 3]|\n|      3|2021-02-23 05:32:29|          DELETE|                  [predicate -> [\"((`id` % CAST(2 AS BIGINT)) = CAST(0 AS BIGINT))\"]]|          2|                                                                                                                           [numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 0]|\n|      2|2021-02-23 05:32:24|          UPDATE|                   [predicate -> ((id#505L % cast(2 as bigint)) = cast(0 as bigint))]|          1|                                                                                                                           [numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 0]|\n|      1|2021-02-23 05:32:00|           WRITE|                                               [mode -> Overwrite, partitionBy -> []]|          0|                                                                                                                                                   [numFiles -> 6, numOutputBytes -> 2407, numOutputRows -> 5]|\n|      0|2021-02-23 05:31:39|           WRITE|                                           [mode -> ErrorIfExists, partitionBy -> []]|       null|                                                                                                                                                   [numFiles -> 6, numOutputBytes -> 2407, numOutputRows -> 5]|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "stream.stop()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 21,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.7008471Z",
              "execution_start_time": "2021-02-23T05:33:02.9652369Z",
              "execution_finish_time": "2021-02-23T05:33:05.0045411Z"
            },
            "text/plain": "StatementMeta(small, 8, 21, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.7277187Z",
              "execution_start_time": "2021-02-23T05:33:05.0738409Z",
              "execution_finish_time": "2021-02-23T05:33:09.144476Z"
            },
            "text/plain": "StatementMeta(small, 8, 22, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|version|          timestamp|       operation|                                                                  operationParameters|readVersion|                                                                                                                                                                                              operationMetrics|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|      7|2021-02-23 05:33:02|STREAMING UPDATE|[outputMode -> Append, queryId -> 348def45-706c-4555-ab12-178864ea7338, epochId -> 2]|          6|                                                                                                                        [numRemovedFiles -> 0, numOutputRows -> 3, numOutputBytes -> 1287, numAddedFiles -> 3]|\n|      6|2021-02-23 05:32:59|STREAMING UPDATE|[outputMode -> Append, queryId -> 348def45-706c-4555-ab12-178864ea7338, epochId -> 1]|          5|                                                                                                                        [numRemovedFiles -> 0, numOutputRows -> 3, numOutputBytes -> 1287, numAddedFiles -> 3]|\n|      5|2021-02-23 05:32:56|STREAMING UPDATE|[outputMode -> Append, queryId -> 348def45-706c-4555-ab12-178864ea7338, epochId -> 0]|          4|                                                                                                                         [numRemovedFiles -> 0, numOutputRows -> 0, numOutputBytes -> 262, numAddedFiles -> 1]|\n|      4|2021-02-23 05:32:40|           MERGE|                                         [predicate -> (oldData.`id` = newData.`id`)]|          3|[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 21, numTargetRowsInserted -> 17, numTargetRowsUpdated -> 3, numOutputRows -> 20, numSourceRows -> 20, numTargetFilesRemoved -> 3]|\n|      3|2021-02-23 05:32:29|          DELETE|                  [predicate -> [\"((`id` % CAST(2 AS BIGINT)) = CAST(0 AS BIGINT))\"]]|          2|                                                                                                                           [numRemovedFiles -> 2, numDeletedRows -> 2, numAddedFiles -> 1, numCopiedRows -> 0]|\n|      2|2021-02-23 05:32:24|          UPDATE|                   [predicate -> ((id#505L % cast(2 as bigint)) = cast(0 as bigint))]|          1|                                                                                                                           [numRemovedFiles -> 2, numAddedFiles -> 2, numUpdatedRows -> 2, numCopiedRows -> 0]|\n|      1|2021-02-23 05:32:00|           WRITE|                                               [mode -> Overwrite, partitionBy -> []]|          0|                                                                                                                                                   [numFiles -> 6, numOutputBytes -> 2407, numOutputRows -> 5]|\n|      0|2021-02-23 05:31:39|           WRITE|                                           [mode -> ErrorIfExists, partitionBy -> []]|       null|                                                                                                                                                   [numFiles -> 6, numOutputBytes -> 2407, numOutputRows -> 5]|\n+-------+-------------------+----------------+-------------------------------------------------------------------------------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compaction\n",
        "\n",
        "If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
        "\n",
        "The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "partition_count = 2\n",
        "\n",
        "spark.read\\\n",
        "    .format(\"delta\")\\\n",
        "    .load(delta_table_path)\\\n",
        "    .repartition(partition_count)\\\n",
        "    .write.option(\"dataChange\", \"false\")\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .save(delta_table_path)    "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.7589925Z",
              "execution_start_time": "2021-02-23T05:33:09.2084583Z",
              "execution_finish_time": "2021-02-23T05:33:13.282982Z"
            },
            "text/plain": "StatementMeta(small, 8, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Parquet to Delta\n",
        "You can do an in-place conversion from the Parquet format to Delta."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
        "\n",
        "data = spark.range(0,5)\n",
        "data.write.parquet(parquet_path)\n",
        "\n",
        "# Confirm that the data isn't in the Delta format\n",
        "DeltaTable.isDeltaTable(spark, parquet_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.7858231Z",
              "execution_start_time": "2021-02-23T05:33:13.3440872Z",
              "execution_finish_time": "2021-02-23T05:33:15.3850601Z"
            },
            "text/plain": "StatementMeta(small, 8, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "False"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
        "\n",
        "# Confirm that the converted data is now in the Delta format\n",
        "DeltaTable.isDeltaTable(spark, parquet_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.8122425Z",
              "execution_start_time": "2021-02-23T05:33:15.4466781Z",
              "execution_finish_time": "2021-02-23T05:33:23.6025483Z"
            },
            "text/plain": "StatementMeta(small, 8, 25, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Support\n",
        "Delta supports table utility commands through SQL.  You can use SQL to:\n",
        "* Get a DeltaTable's history\n",
        "* Vacuum a DeltaTable\n",
        "* Convert a Parquet file to Delta\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.8361807Z",
              "execution_start_time": "2021-02-23T05:33:23.684799Z",
              "execution_finish_time": "2021-02-23T05:33:25.7228079Z"
            },
            "text/plain": "StatementMeta(small, 8, 26, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "+-------+-------------------+------+--------+----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+\n|version|          timestamp|userId|userName|       operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+\n|      8|2021-02-23 05:33:11|  null|    null|           WRITE|[mode -> Overwrit...|null|    null|     null|          7|          null|        false|[numFiles -> 2, n...|\n|      7|2021-02-23 05:33:02|  null|    null|STREAMING UPDATE|[outputMode -> Ap...|null|    null|     null|          6|          null|         true|[numRemovedFiles ...|\n|      6|2021-02-23 05:32:59|  null|    null|STREAMING UPDATE|[outputMode -> Ap...|null|    null|     null|          5|          null|         true|[numRemovedFiles ...|\n|      5|2021-02-23 05:32:56|  null|    null|STREAMING UPDATE|[outputMode -> Ap...|null|    null|     null|          4|          null|         true|[numRemovedFiles ...|\n|      4|2021-02-23 05:32:40|  null|    null|           MERGE|[predicate -> (ol...|null|    null|     null|          3|          null|        false|[numTargetRowsCop...|\n|      3|2021-02-23 05:32:29|  null|    null|          DELETE|[predicate -> [\"(...|null|    null|     null|          2|          null|        false|[numRemovedFiles ...|\n|      2|2021-02-23 05:32:24|  null|    null|          UPDATE|[predicate -> ((i...|null|    null|     null|          1|          null|        false|[numRemovedFiles ...|\n|      1|2021-02-23 05:32:00|  null|    null|           WRITE|[mode -> Overwrit...|null|    null|     null|          0|          null|        false|[numFiles -> 6, n...|\n|      0|2021-02-23 05:31:39|  null|    null|           WRITE|[mode -> ErrorIfE...|null|    null|     null|       null|          null|         true|[numFiles -> 6, n...|\n+-------+-------------------+------+--------+----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.862963Z",
              "execution_start_time": "2021-02-23T05:33:25.7884175Z",
              "execution_finish_time": "2021-02-23T05:34:18.6026171Z"
            },
            "text/plain": "StatementMeta(small, 8, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "+--------------------+\n|                path|\n+--------------------+\n|abfss://zhaotest@...|\n+--------------------+"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_id = random.randint(0,1000)\n",
        "parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
        "\n",
        "data = spark.range(0,5)\n",
        "data.write.parquet(parquet_path)\n",
        "\n",
        "# Confirm that the data isn't in the Delta format\n",
        "DeltaTable.isDeltaTable(spark, parquet_path)\n",
        "\n",
        "# Use SQL to convert the parquet table to Delta\n",
        "spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
        "\n",
        "DeltaTable.isDeltaTable(spark, parquet_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "small",
              "session_id": 8,
              "statement_id": 28,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-02-23T05:30:38.8863961Z",
              "execution_start_time": "2021-02-23T05:34:18.6732815Z",
              "execution_finish_time": "2021-02-23T05:34:24.8572968Z"
            },
            "text/plain": "StatementMeta(small, 8, 28, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}